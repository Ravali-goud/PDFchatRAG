{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravali-goud/PDFchatRAG/blob/main/Chat_with_Website_Using_RAG_Pipeline_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Chat with Website Using RAG Pipeline"
      ],
      "metadata": {
        "id": "hbnFbA9QwDVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project creates a system that automatically extracts content from websites, stores it in a searchable format, and retrieves relevant information based on user queries.\n",
        "\n"
      ],
      "metadata": {
        "id": "FKzT3MdhwH-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ciz-fyAqPm5G"
      },
      "outputs": [],
      "source": [
        "!pip install requests beautifulsoup4 sentence-transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import hashlib\n",
        "import hmac\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss"
      ],
      "metadata": {
        "id": "Z1TSSZPKRJ7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini API Key Setup\n",
        "GEMINI_API_KEY = \"AIzaSyBq3nsMVcC6eQPjeYURI3nCDrHKOADLeY0\" #Use your own API"
      ],
      "metadata": {
        "id": "n-44-JjeRU4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gemini API Authentication\n",
        "def authenticate_gemini():\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"X-GEMINI-APIKEY\": GEMINI_API_KEY,\n",
        "    }\n",
        "    response = requests.get(\"https://api.gemini.com/v1/pubticker/btcusd\", headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Authenticated successfully with Gemini API!\")\n",
        "    else:\n",
        "        raise Exception(f\"Authentication failed: {response.json()}\")"
      ],
      "metadata": {
        "id": "u41PARdgRYZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape website using requests\n",
        "def scrape_website(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        text = \" \".join([para.get_text() for para in paragraphs])\n",
        "        return text\n",
        "    else:\n",
        "        raise Exception(f\"Failed to fetch {url}: {response.status_code}\")"
      ],
      "metadata": {
        "id": "GTj4jXh8RcLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape website using Selenium for JavaScript-rendered content\n",
        "def scrape_website_with_selenium(url):\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    service = Service(\"/usr/bin/chromedriver\")\n",
        "    driver = webdriver.Chrome(service=service, options=options)\n",
        "    driver.get(url)\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    text = \" \".join([para.get_text() for para in paragraphs])\n",
        "    driver.quit()\n",
        "    return text"
      ],
      "metadata": {
        "id": "GW3eEwqZRnd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking the text\n",
        "def chunk_text(text, chunk_size=300):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]"
      ],
      "metadata": {
        "id": "qDLJOFaVRvqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding and storing in FAISS\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def embed_and_store(chunks):\n",
        "    embeddings = model.encode(chunks)\n",
        "    dimension = embeddings.shape[1]\n",
        "    vector_db = faiss.IndexFlatL2(dimension)\n",
        "    vector_db.add(embeddings)\n",
        "    return vector_db, embeddings"
      ],
      "metadata": {
        "id": "hUBKXi2NRzJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Handling\n",
        "def query_vector_search(query, vector_db, chunks, top_k=5):\n",
        "    query_vec = model.encode([query])\n",
        "    distances, indices = vector_db.search(query_vec, top_k)\n",
        "    results = [chunks[i] for i in indices[0]]\n",
        "    return results"
      ],
      "metadata": {
        "id": "Tm-RgLn3R2U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a simple response\n",
        "def generate_response(query, context):\n",
        "    response = f\"Query: {query}\\n\\nRelevant Context:\\n{context}\"\n",
        "    return response"
      ],
      "metadata": {
        "id": "MuLUOc2qTlwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Workflow\n",
        "try:\n",
        "    # Authenticate with Gemini\n",
        "    authenticate_gemini()\n",
        "\n",
        "    # List of URLs to scrape\n",
        "    urls = [\n",
        "        \"https://www.uchicago.edu/\",\n",
        "        \"https://www.washington.edu/\",\n",
        "        \"https://www.stanford.edu/\",\n",
        "        \"https://und.edu/\"\n",
        "    ]\n",
        "\n",
        "    # Scrape content from websites\n",
        "    all_chunks = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            print(f\"Scraping: {url}\")\n",
        "            website_text = scrape_website(url)\n",
        "        except Exception as e:\n",
        "            print(f\"Using Selenium for: {url}\")\n",
        "            website_text = scrape_website_with_selenium(url)\n",
        "        chunks = chunk_text(website_text)\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    # Store embeddings in FAISS\n",
        "    vector_db, embeddings = embed_and_store(all_chunks)\n",
        "    # Query the system\n",
        "    query = \"What are the main features of Stanford University?\"\n",
        "    retrieved_chunks = query_vector_search(query, vector_db, all_chunks)\n",
        "    context = \"\\n\".join(retrieved_chunks)\n",
        "\n",
        "    # Generate response\n",
        "    response = generate_response(query, context)\n",
        "    print(\"\\nResponse:\")\n",
        "    print(response)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Error occurred:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "mzPPjanbTqfm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}